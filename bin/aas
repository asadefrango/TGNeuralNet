FANN_FLO_2.1
num_layers=2
learning_rate=0.700000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 9 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) 
connections (connected_to_neuron, weight)=(0, -7.22312554717063903809e-02) (1, 2.86599472165107727051e-02) (2, 7.32337012887001037598e-02) (3, 7.61110559105873107910e-02) (4, 2.74412706494331359863e-02) (5, 9.92128625512123107910e-02) (6, 9.30506661534309387207e-02) (7, -8.22832062840461730957e-02) (8, -7.40496814250946044922e-03) (9, -3.55478450655937194824e-02) (10, -9.74881798028945922852e-02) (0, -8.36025774478912353516e-02) (1, 4.90896925330162048340e-02) (2, 8.78028348088264465332e-02) (3, -2.38905623555183410645e-02) (4, 8.48471894860267639160e-02) (5, 5.07105365395545959473e-02) (6, 8.45161899924278259277e-02) (7, -2.06183642148971557617e-02) (8, -2.52910852432250976562e-02) (9, -5.69562427699565887451e-02) (10, 8.85689780116081237793e-02) (0, 1.88793540000915527344e-02) (1, 6.89440891146659851074e-02) (2, -8.25006961822509765625e-02) (3, -9.66394096612930297852e-02) (4, 6.33625909686088562012e-02) (5, 5.63438460230827331543e-02) (6, 8.72449204325675964355e-02) (7, 4.25912365317344665527e-02) (8, 8.96208509802818298340e-02) (9, -8.49863365292549133301e-02) (10, -2.87488102912902832031e-02) (0, 6.28545507788658142090e-02) (1, 9.11247208714485168457e-02) (2, 9.86924842000007629395e-02) (3, 6.20674118399620056152e-02) (4, 8.41753855347633361816e-02) (5, -8.35907310247421264648e-02) (6, -4.53375577926635742188e-02) (7, -5.13724610209465026855e-02) (8, -8.10789167881011962891e-02) (9, -2.89401337504386901855e-02) (10, 9.77172330021858215332e-02) (0, -9.32760834693908691406e-02) (1, 4.71693053841590881348e-02) (2, 8.25644209980964660645e-02) (3, 5.74344471096992492676e-02) (4, 3.16854938864707946777e-02) (5, -3.80539372563362121582e-02) (6, -6.78566396236419677734e-02) (7, 7.47292563319206237793e-02) (8, -4.94849644601345062256e-02) (9, 5.10227158665657043457e-02) (10, 4.36733439564704895020e-02) (0, -3.19856554269790649414e-02) (1, 5.43833151459693908691e-02) (2, 7.03592598438262939453e-03) (3, -7.56418257951736450195e-02) (4, 4.16282340884208679199e-02) (5, -5.03728389739990234375e-02) (6, -8.60209688544273376465e-02) (7, 5.66418841481208801270e-02) (8, 2.08783522248268127441e-02) (9, 7.68335834145545959473e-02) (10, 4.77666035294532775879e-02) (0, 1.95708274841308593750e-02) (1, 3.89009937644004821777e-02) (2, 3.19420024752616882324e-02) (3, 3.59800979495048522949e-02) (4, 9.35634449124336242676e-02) (5, 8.05695280432701110840e-02) (6, 5.49011901021003723145e-02) (7, -3.53766903281211853027e-02) (8, 7.82867744565010070801e-02) (9, 6.16251006722450256348e-02) (10, -8.82073864340782165527e-02) (0, 6.08511939644813537598e-02) (1, 1.90595388412475585938e-02) (2, 4.34781089425086975098e-02) (3, -7.72027522325515747070e-02) (4, 5.12029007077217102051e-02) (5, 1.82073563337326049805e-02) (6, -2.66877114772796630859e-02) (7, 2.22562253475189208984e-03) (8, -3.81192974746227264404e-02) (9, 4.13266345858573913574e-02) (10, -4.33910675346851348877e-02) (0, 6.89166262745857238770e-02) (1, 6.56848028302192687988e-02) (2, 9.82371643185615539551e-02) (3, -8.14562067389488220215e-02) (4, 7.96638354659080505371e-02) (5, 5.48790469765663146973e-02) (6, 3.94221469759941101074e-02) (7, 5.64974322915077209473e-02) (8, 2.64565646648406982422e-03) (9, -4.10070307552814483643e-02) (10, -4.60158288478851318359e-03) 
