FANN_FLO_2.1
num_layers=2
learning_rate=0.700000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 9 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) (11, 0, 5.00000000000000000000e-01) 
connections (connected_to_neuron, weight)=(0, -2.13109180331230163574e-02) (1, -1.38525590300559997559e-02) (2, 3.70210334658622741699e-02) (3, 5.67691251635551452637e-02) (4, 4.36250492930412292480e-02) (5, 9.67134758830070495605e-02) (6, -1.01999938488006591797e-02) (7, -2.86512151360511779785e-02) (8, -3.58469486236572265625e-02) (9, -8.06455314159393310547e-02) (10, 8.61202180385589599609e-03) (0, 9.59892347455024719238e-02) (1, 6.49617612361907958984e-03) (2, 9.75803658366203308105e-02) (3, 9.34829637408256530762e-02) (4, -7.00503438711166381836e-02) (5, -3.48200872540473937988e-02) (6, -2.68568098545074462891e-03) (7, -2.37116813659667968750e-02) (8, -5.20356521010398864746e-02) (9, -9.58468988537788391113e-02) (10, -7.86044374108314514160e-02) (0, 6.84995576739311218262e-02) (1, 2.49324217438697814941e-02) (2, -7.99215957522392272949e-02) (3, -5.41238859295845031738e-02) (4, -4.16849628090858459473e-02) (5, 6.34445026516914367676e-02) (6, 4.15972098708152770996e-02) (7, -8.93555656075477600098e-02) (8, 6.65049031376838684082e-02) (9, -7.97137171030044555664e-02) (10, -3.20811569690704345703e-03) (0, 3.52593511343002319336e-03) (1, 7.70554021000862121582e-02) (2, -5.95830790698528289795e-02) (3, 2.39409506320953369141e-04) (4, -3.31445783376693725586e-02) (5, 1.17657184600830078125e-02) (6, 6.43924549221992492676e-02) (7, -1.37901157140731811523e-02) (8, -7.96222537755966186523e-02) (9, 6.03816881775856018066e-02) (10, 9.27060618996620178223e-02) (0, -8.20419043302536010742e-02) (1, 5.38646504282951354980e-02) (2, -7.73442760109901428223e-02) (3, -1.68619900941848754883e-02) (4, -4.88210394978523254395e-02) (5, -1.05595588684082031250e-03) (6, 3.11023667454719543457e-02) (7, -4.46679368615150451660e-02) (8, 2.03396081924438476562e-02) (9, -3.98084521293640136719e-04) (10, 8.02645012736320495605e-02) (0, 4.04180064797401428223e-02) (1, 4.54780384898185729980e-02) (2, -6.14204667508602142334e-02) (3, 3.86252254247665405273e-03) (4, -1.29247605800628662109e-02) (5, -5.07760308682918548584e-02) (6, -2.96325758099555969238e-02) (7, 7.36153125762939453125e-03) (8, 4.60158511996269226074e-02) (9, 7.38933607935905456543e-02) (10, -1.55830606818199157715e-02) (0, 8.64327773451805114746e-02) (1, -2.58672386407852172852e-02) (2, 5.12723550200462341309e-02) (3, -1.80151313543319702148e-03) (4, -6.14747777581214904785e-02) (5, -6.25177621841430664062e-02) (6, 1.85762271285057067871e-02) (7, 9.89069119095802307129e-02) (8, -6.98116943240165710449e-02) (9, 3.65343317389488220215e-02) (10, 5.27715608477592468262e-02) (0, -4.71559725701808929443e-02) (1, -8.03276598453521728516e-02) (2, -9.60494801402091979980e-02) (3, 5.17880842089653015137e-02) (4, 5.07747009396553039551e-02) (5, -4.07174117863178253174e-02) (6, -2.78723165392875671387e-02) (7, -4.96233701705932617188e-02) (8, -6.04529194533824920654e-02) (9, -8.74543115496635437012e-02) (10, 9.58546623587608337402e-02) (0, -2.18733847141265869141e-02) (1, 1.64082199335098266602e-02) (2, -1.70700997114181518555e-02) (3, 2.73505821824073791504e-02) (4, 8.67756381630897521973e-02) (5, 9.02914330363273620605e-02) (6, -2.66335606575012207031e-02) (7, 6.06690123677253723145e-02) (8, -2.52916291356086730957e-02) (9, -4.02007885277271270752e-02) (10, -6.51982352137565612793e-02) 
